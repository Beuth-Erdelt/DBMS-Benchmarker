@InProceedings{10.1007/978-3-030-84924-5_6,
author="Erdelt, Patrick K.",
editor="Nambiar, Raghunath
and Poess, Meikel",
title="A Framework for Supporting Repetition and Evaluation in the Process of Cloud-Based DBMS Performance Benchmarking",
booktitle="Performance Evaluation and Benchmarking",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="75--92",
abstract="Performance benchmarking of Database Management Systems (DBMS) is an important yet complicated process. We motivate and present two supporting Python packages which help to avoid common pitfalls and in particular improve reproducibility and transparency in heterogeneous systems with hardware accelerators. The first addresses operational aspects by providing dynamic testbeds using Docker images, especially for cloud-based systems. The second helps planning and recurrently running experiments in a predefined setup via JDBC/SQL, and analyzing results with automated reports and an interactive dashboard. The purpose of this is to thoroughly evaluate aspects of performances of DBMS based on real-life measurements, runtime and hardware metrics, depending on various parameters including the hardware, and with high repeatability. We present a series of TPC-H inspired example benchmarks in a Kubernetes cluster for demonstration, and some lessons learned.",
isbn="978-3-030-84924-5"
}