

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Evaluation &mdash; DBMSBenchmarker 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> DBMSBenchmarker
          

          
          </a>

          
            
            
              <div class="version">
                0.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Evaluation</a><ul>
<li><a class="reference internal" href="#featured-evaluations">Featured Evaluations</a><ul>
<li><a class="reference internal" href="#informations-about-dbms">Informations about DBMS</a><ul>
<li><a class="reference internal" href="#throughput-and-latency">Throughput and Latency</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id5">Global Metrics</a><ul>
<li><a class="reference internal" href="#id6">Latency and Throughput</a></li>
<li><a class="reference internal" href="#average-ranking">Average Ranking</a></li>
<li><a class="reference internal" href="#time-of-ingest-per-dbms">Time of Ingest per DBMS</a></li>
<li><a class="reference internal" href="#id7">Hardware Metrics</a></li>
<li><a class="reference internal" href="#id8">Host Metrics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10">Drill-Down Timers</a><ul>
<li><a class="reference internal" href="#relative-ranking-based-on-times">Relative Ranking based on Times</a></li>
<li><a class="reference internal" href="#id11">Average Times</a></li>
</ul>
</li>
<li><a class="reference internal" href="#slice-timers">Slice Timers</a><ul>
<li><a class="reference internal" href="#id12">Heatmap of Factors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id13">Drill-Down Queries</a><ul>
<li><a class="reference internal" href="#id14">Total Times</a></li>
<li><a class="reference internal" href="#id15">Normalized Total Times</a></li>
<li><a class="reference internal" href="#id16">Throughputs</a></li>
<li><a class="reference internal" href="#id17">Latencies</a></li>
<li><a class="reference internal" href="#id18">Sizes of Result Sets</a></li>
<li><a class="reference internal" href="#id19">Errors</a></li>
<li><a class="reference internal" href="#id20">Warnings</a></li>
</ul>
</li>
<li><a class="reference internal" href="#slice-queries">Slice Queries</a><ul>
<li><a class="reference internal" href="#latency-and-throughput-per-query">Latency and Throughput per Query</a></li>
<li><a class="reference internal" href="#hardware-metrics-per-query">Hardware Metrics per Query</a></li>
<li><a class="reference internal" href="#timers-per-query">Timers Per Query</a></li>
</ul>
</li>
<li><a class="reference internal" href="#slice-queries-and-timers">Slice Queries and Timers</a><ul>
<li><a class="reference internal" href="#statistics-table">Statistics Table</a></li>
<li><a class="reference internal" href="#plot-of-values">Plot of Values</a></li>
<li><a class="reference internal" href="#boxplot-of-values">Boxplot of Values</a></li>
<li><a class="reference internal" href="#histogram-of-values">Histogram of Values</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id22">Further Data</a><ul>
<li><a class="reference internal" href="#result-sets-per-query">Result Sets per Query</a></li>
<li><a class="reference internal" href="#all-benchmark-times">All Benchmark Times</a></li>
<li><a class="reference internal" href="#all-errors">All Errors</a></li>
<li><a class="reference internal" href="#all-warnings">All Warnings</a></li>
<li><a class="reference internal" href="#id23">Initialization Scripts</a></li>
<li><a class="reference internal" href="#bexhoma-workflow">Bexhoma Workflow</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DBMSBenchmarker</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Evaluation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/Evaluations.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h1>
<p>After an experiment has finished, the results can be evaluated</p>
<ul class="simple">
<li><p>with an interactive <a class="reference external" href="Dashboard.html">dashboard</a></p></li>
<li><p>in a Latex report containing most of the results</p></li>
<li><p>with an interactive <a class="reference external" href="Inspection.html">inspection module</a></p></li>
</ul>
<p>There is an <em>evaluator class</em>, which collects most of the (numerical) evaluations and provides them as an <strong>evaluation dict</strong>.</p>
<div class="section" id="featured-evaluations">
<h2>Featured Evaluations<a class="headerlink" href="#featured-evaluations" title="Permalink to this headline">¶</a></h2>
<p>Predefined evaluations are</p>
<ul class="simple">
<li><p><a class="reference external" href="#global-metrics">Global Metrics</a></p>
<ul>
<li><p><a class="reference external" href="#average-ranking">average position</a></p></li>
<li><p><a class="reference external" href="#latency-and-throughput">latency and throughput</a></p></li>
<li><p><a class="reference external" href="#time-of-ingest-per-dbms">ingestion</a></p></li>
<li><p><a class="reference external" href="#hardware-metrics">hardware metrics</a></p></li>
<li><p><a class="reference external" href="#host-metrics">host metrics</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#drill-down-timers">Drill-Down Timers</a></p>
<ul>
<li><p><a class="reference external" href="#relative-ranking-based-on-times">relative position</a></p></li>
<li><p><a class="reference external" href="#average-times">average times</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#slice-timers">Slices of Timers</a></p>
<ul>
<li><p><a class="reference external" href="#heatmap-of-factors">heatmap of factors</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#drill-down-queries">Drill-Down Queries</a></p>
<ul>
<li><p><a class="reference external" href="#total-times">total times</a></p></li>
<li><p><a class="reference external" href="#normalized-total-times">normalized total times</a></p></li>
<li><p><a class="reference external" href="#latencies">latencies</a></p></li>
<li><p><a class="reference external" href="#throughputs">throughputs</a></p></li>
<li><p><a class="reference external" href="#sizes-of-result-sets">sizes of result sets</a></p></li>
<li><p><a class="reference external" href="#errors">errors</a></p></li>
<li><p><a class="reference external" href="#warnings">warnings</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#slice-queries">Slices of Queries</a></p>
<ul>
<li><p><a class="reference external" href="#latency-and-throughput-per-query">latency and throughput</a></p></li>
<li><p><a class="reference external" href="#hardware-metrics-per-query">hardware metrics</a></p></li>
<li><p><a class="reference external" href="#timers-per-query">timers</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#slice-queries-and-timers">Slices of Queries and Timers</a></p>
<ul>
<li><p><a class="reference external" href="#statistics-table">statistics</a> - measures of tendency and dispersion, sensitive and insensitive to outliers</p></li>
<li><p><a class="reference external" href="#plot-of-values">plots</a> of times</p></li>
<li><p><a class="reference external" href="#boxplot-of-values">box plots</a> of times</p></li>
</ul>
</li>
<li><p>summarizing and exhaustive latex reports containing <a class="reference external" href="#further-data">further data</a> like</p>
<ul>
<li><p>precision and identity checks of <a class="reference external" href="#comparison">result sets</a></p></li>
<li><p><a class="reference external" href="#all-errors">error messages</a></p></li>
<li><p><a class="reference external" href="#all-warnings">warnings</a></p></li>
<li><p><a class="reference external" href="#all-benchmark-times">benchmark times</a></p></li>
<li><p><a class="reference external" href="#bexhoma-workflow">experiment workflow</a></p></li>
<li><p><a class="reference external" href="#initialization-scripts">initialization scripts</a></p></li>
</ul>
</li>
<li><p>an interactive <a class="reference external" href="#inspector">inspection tool</a></p></li>
<li><p>a Latex report containing most of these</p></li>
</ul>
<div class="section" id="informations-about-dbms">
<h3>Informations about DBMS<a class="headerlink" href="#informations-about-dbms" title="Permalink to this headline">¶</a></h3>
<p>This evaluation is available in the evaluation dict and in the latex reports.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/dbms.png" width="480">
</p><p>The user has to provide in a <a class="reference external" href="Options.html#connection-file">config file</a></p>
<ul class="simple">
<li><p>a unique name (<strong>connectionname</strong>)</p></li>
<li><p>JDBC connection information</p></li>
</ul>
<p>If a monitoring interface is provided, <a class="reference external" href="Concept.html#monitoring-hardware-metrics">hardware metrics</a> are collected and aggregated.
We may further provide describing information for reporting.</p>
<div class="section" id="throughput-and-latency">
<h4>Throughput and Latency<a class="headerlink" href="#throughput-and-latency" title="Permalink to this headline">¶</a></h4>
<p>The abbreviations mean</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lat_r</span> <span class="o">=</span> <span class="n">Latency</span> <span class="n">of</span> <span class="n">runs</span> <span class="p">(</span><span class="n">mean</span> <span class="n">time</span><span class="p">)</span> <span class="p">[</span><span class="n">ms</span><span class="p">]</span>
<span class="n">lat_s</span> <span class="o">=</span> <span class="n">Latency</span> <span class="n">of</span> <span class="n">session</span> <span class="p">(</span><span class="n">mean</span> <span class="n">time</span><span class="p">)</span> <span class="p">[</span><span class="n">ms</span><span class="p">]</span>
<span class="n">tps_r1</span> <span class="o">=</span> <span class="n">Throughput</span> <span class="n">of</span> <span class="n">runs</span> <span class="p">(</span><span class="n">number</span> <span class="n">of</span> <span class="n">runs</span> <span class="o">/</span> <span class="n">total</span> <span class="n">time</span><span class="p">)</span> <span class="p">[</span><span class="n">Hz</span><span class="p">]</span>
<span class="n">tps_r2</span> <span class="o">=</span> <span class="n">Throughput</span> <span class="n">of</span> <span class="n">runs</span> <span class="p">(</span><span class="n">number</span> <span class="n">of</span> <span class="n">parallel</span> <span class="n">clients</span> <span class="o">/</span> <span class="n">cleaned</span> <span class="n">mean</span> <span class="n">time</span><span class="p">)</span> <span class="p">[</span><span class="n">Hz</span><span class="p">]</span>
<span class="n">tps_s1</span> <span class="o">=</span> <span class="n">Throughput</span> <span class="n">of</span> <span class="n">sessions</span> <span class="p">(</span><span class="n">number</span> <span class="n">of</span> <span class="n">runs</span> <span class="o">/</span> <span class="n">length</span> <span class="n">of</span> <span class="n">sessions</span> <span class="o">/</span> <span class="n">total</span> <span class="n">time</span><span class="p">)</span> <span class="p">[</span><span class="n">Hz</span><span class="p">]</span>
<span class="n">tps_s2</span> <span class="o">=</span> <span class="n">Throughput</span> <span class="n">of</span> <span class="n">sessions</span> <span class="p">(</span><span class="n">number</span> <span class="n">of</span> <span class="n">parallel</span> <span class="n">clients</span> <span class="o">/</span> <span class="n">cleaned</span> <span class="n">mean</span> <span class="n">time</span><span class="p">)</span> <span class="p">[</span><span class="n">Hz</span><span class="p">]</span>
<span class="n">tph_r2</span> <span class="o">=</span> <span class="n">Throughput</span> <span class="n">of</span> <span class="n">runs</span> <span class="p">(</span><span class="n">tps_r2</span> <span class="o">*</span> <span class="mi">3600</span><span class="p">)</span> <span class="p">[</span><span class="n">pH</span><span class="p">]</span>
</pre></div>
</div>
<p>The metrics of index 2 are based on the assumption that the number of clients equals the size of the queues. To check this, there are another metrics:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">qs_r</span> <span class="o">=</span> <span class="n">Queue</span> <span class="n">size</span> <span class="n">of</span> <span class="n">runs</span> <span class="p">(</span><span class="n">tps_r1</span> <span class="o">*</span> <span class="n">lat_r</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">qs_s</span> <span class="o">=</span> <span class="n">Queue</span> <span class="n">size</span> <span class="n">of</span> <span class="n">sessions</span> <span class="p">(</span><span class="n">tps_s1</span> <span class="o">*</span> <span class="n">lat_s</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Note</strong> that the total times include some overhead like spawning a pool of subprocesses, so these metrics are also a measurement of overhead.</p>
</div>
</div>
<div class="section" id="id5">
<h3>Global Metrics<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id6">
<h4>Latency and Throughput<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/relative-tps-lat.png" width="640">
</p><p>For each query, latency and throughput is computed per DBMS.
This chart shows the geometric mean over all queries and per DBMS.
Only successful queries and DBMS not producing any error are considered there.</p>
</div>
<div class="section" id="average-ranking">
<h4>Average Ranking<a class="headerlink" href="#average-ranking" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/ranking.png" width="480">
</p><p>We compute a ranking of DBMS for each query based on the sum of times, from fastest to slowest.
Unsuccessful DBMS are considered last place.
The chart shows the average ranking per DBMS.</p>
</div>
<div class="section" id="time-of-ingest-per-dbms">
<h4>Time of Ingest per DBMS<a class="headerlink" href="#time-of-ingest-per-dbms" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/total_barh_ingest.png" width="480">
</p><p>This is part of the informations provided by the user.
The tool does not measure time of ingest explicitly.</p>
</div>
<div class="section" id="id7">
<h4>Hardware Metrics<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<p>The chart shows the metrics obtained from monitoring.
Values are computed as arithmetic mean across benchmarking time.
Only successful queries and DBMS not producing any error are considered.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/monitoring-metrics.png" width="640">
</p></div>
<div class="section" id="id8">
<h4>Host Metrics<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<p>The chart shows the metrics obtained from inside docker containers.
The host information is provided in the <a class="reference external" href="#connection-file">config file</a>.
Here, cost is based on the total time.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/host-metrics.png" width="640">
</p></div>
</div>
<div class="section" id="id10">
<h3>Drill-Down Timers<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<div class="section" id="relative-ranking-based-on-times">
<h4>Relative Ranking based on Times<a class="headerlink" href="#relative-ranking-based-on-times" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/relative.png" width="480">
</p><p>For each query and timer, the best DBMS is considered as gold standard = 100%. Based on their times, the other DBMS obtain a relative ranking factor.
Only successful queries and DBMS not producing any error are considered.
The chart shows the geometric mean of factors per DBMS.</p>
</div>
<div class="section" id="id11">
<h4>Average Times<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/sum_of_times.png" width="480">
</p><p>This is based on the mean times of all benchmark test runs.
Measurements start before each benchmark run and stop after the same benchmark run has been finished. The average value is computed per query.
Parallel benchmark runs should not slow down in an ideal situation.
Only successful queries and DBMS not producing any error are considered.
The chart shows the average of query times based on mean values per DBMS and per timer.</p>
<p><strong>Note</strong> that the mean of mean values (here) is in general not the same as mean of all runs (different queries may have different number of runs).</p>
</div>
</div>
<div class="section" id="slice-timers">
<h3>Slice Timers<a class="headerlink" href="#slice-timers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id12">
<h4>Heatmap of Factors<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/heatmap-timer.png" width="480">
</p><p>The relative ranking can be refined to see the contribution of each query.
The chart shows the factor of the corresponding timer per query and DBMS.
All active queries and DBMS are considered.</p>
</div>
</div>
<div class="section" id="id13">
<h3>Drill-Down Queries<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id14">
<h4>Total Times<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/total_times.png" width="480">
</p><p>This is based on the times each DBMS is queried in total. Measurement starts before first benchmark run and stops after the last benchmark run has been finished. Parallel benchmarks should speed up the total time in an ideal situation.
Only successful queries and DBMS not producing any error are considered.
Note this also includes the time needed for sorting and storing result sets etc.
The chart shows the total query time per DBMS and query.</p>
</div>
<div class="section" id="id15">
<h4>Normalized Total Times<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/total_times-norm.png" width="480">
</p><p>The chart shows total times per query, normalized to the average total time of that query.
Only successful queries and DBMS not producing any error are considered.
This is also available as a heatmap.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/total_times-heatmap.png" width="480">
</p></div>
<div class="section" id="id16">
<h4>Throughputs<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/tps-heatmap.png" width="480">
</p><p>For each query, latency and throughput is computed per DBMS.
The chart shows tps_r2.
Only successful queries and DBMS not producing any error are considered there.</p>
</div>
<div class="section" id="id17">
<h4>Latencies<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/lat-heatmap.png" width="480">
</p><p>For each query, latency and throughput is computed per DBMS.
The chart shows lat_r.
Only successful queries and DBMS not producing any error are considered there.</p>
</div>
<div class="section" id="id18">
<h4>Sizes of Result Sets<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/resultsets-heatmap.png" width="480">
</p><p>For each query, the size of received data per DBMS is stored.
The chart shows the size of result sets per DBMS and per timer.
Sizes are normalized to minimum per query.
All active queries and DBMS are considered.</p>
</div>
<div class="section" id="id19">
<h4>Errors<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/errors-heatmap.png" width="480">
</p><p>The chart shows per DBMS and per timer, if an error has occured.
All active queries and DBMS are considered.</p>
</div>
<div class="section" id="id20">
<h4>Warnings<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/warnings-heatmap.png" width="480">
</p><p>The chart shows per DBMS and per timer, if a warning has occured.
All active queries and DBMS are considered.</p>
</div>
</div>
<div class="section" id="slice-queries">
<h3>Slice Queries<a class="headerlink" href="#slice-queries" title="Permalink to this headline">¶</a></h3>
<div class="section" id="latency-and-throughput-per-query">
<h4>Latency and Throughput per Query<a class="headerlink" href="#latency-and-throughput-per-query" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/tps-lat.png" width="640">
</p><p>For each query, latency and throughput is computed per DBMS.
This is available as dataframes, in the evaluation dict and as png files per query.
Only successful queries and DBMS not producing any error are considered there.</p>
</div>
<div class="section" id="hardware-metrics-per-query">
<h4>Hardware Metrics per Query<a class="headerlink" href="#hardware-metrics-per-query" title="Permalink to this headline">¶</a></h4>
<p>These metrics are available as png files and csv files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/hardware-metrics.png" width="640">
</p><p>These metrics are collected from a Prometheus / Grafana stack.
This expects time-synchronized servers.</p>
</div>
<div class="section" id="timers-per-query">
<h4>Timers Per Query<a class="headerlink" href="#timers-per-query" title="Permalink to this headline">¶</a></h4>
<p>These plots are available as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/bar.png" width="480">
</p><p>This is based on the sum of times of all single benchmark test runs.
These charts show the average of times per DBMS based on mean value.
Warmup and cooldown are not included.
If data transfer or connection time is also benchmarked, the chart is stacked.
The bars are ordered ascending.</p>
</div>
</div>
<div class="section" id="slice-queries-and-timers">
<h3>Slice Queries and Timers<a class="headerlink" href="#slice-queries-and-timers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="statistics-table">
<h4>Statistics Table<a class="headerlink" href="#statistics-table" title="Permalink to this headline">¶</a></h4>
<p>These tables are available as dataframes and in the evaluation dict.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/table.png" width="640">
</p><p>These tables show <a class="reference external" href="Concept.html#aggregation-functions">statistics</a> about benchmarking time during the various runs per DBMS as a table.
Warmup and cooldown are not included.
This is for inspection of stability.
A factor column is included.
This is computed as the multiple of the minimum of the mean of benchmark times per DBMS.
The DBMS are ordered ascending by factor.</p>
</div>
<div class="section" id="plot-of-values">
<h4>Plot of Values<a class="headerlink" href="#plot-of-values" title="Permalink to this headline">¶</a></h4>
<p>These plots are available as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/plot.png" width="640">
</p><p>These plots show the variation of benchmarking time during the various runs per DBMS as a plot.
Warmup and cooldown are included and marked as such.
This is for inspection of time dependence.</p>
<p><strong>Note</strong> this is only reliable for non-parallel runs.</p>
</div>
<div class="section" id="boxplot-of-values">
<h4>Boxplot of Values<a class="headerlink" href="#boxplot-of-values" title="Permalink to this headline">¶</a></h4>
<p>These plots are available as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/boxplot.png" width="640">
</p><p>These plots show the variation of benchmarking time during the various runs per DBMS as a boxplot.
Warmup, cooldown and zero (missing) values are not included.
This is for inspection of variation and outliers.</p>
</div>
<div class="section" id="histogram-of-values">
<h4>Histogram of Values<a class="headerlink" href="#histogram-of-values" title="Permalink to this headline">¶</a></h4>
<p>These plots are available as png files.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/Beuth-Erdelt/DBMS-Benchmarker/master/docs/histogram.png" width="640">
</p><p>These plots show the variation of benchmarking time during the various runs per DBMS as a histogram.
The number of bins equals the minimum number of result times.
Warmup, cooldown and zero (missing) values are not included.
This is for inspection of the distribution of times.</p>
</div>
</div>
<div class="section" id="id22">
<h3>Further Data<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<div class="section" id="result-sets-per-query">
<h4>Result Sets per Query<a class="headerlink" href="#result-sets-per-query" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes and csv files.</p>
<p>The result set (sorted values, hashed or pure size) of the first run of each DBMS can be saved per query.
This is for comparison and inspection.</p>
</div>
<div class="section" id="all-benchmark-times">
<h4>All Benchmark Times<a class="headerlink" href="#all-benchmark-times" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dataframes, in the evaluation dict and as csv files.</p>
<p>The benchmark times of all runs of each DBMS can be saved per query.
This is for comparison and inspection.</p>
</div>
<div class="section" id="all-errors">
<h4>All Errors<a class="headerlink" href="#all-errors" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dicts.</p>
<p>The errors that may have occured are saved for each DBMS and per query.
The error messages are fetched from Python exceptions thrown during a benchmark run.
This is for inspection of problems.</p>
</div>
<div class="section" id="all-warnings">
<h4>All Warnings<a class="headerlink" href="#all-warnings" title="Permalink to this headline">¶</a></h4>
<p>This evaluation is available as dicts.</p>
<p>The warnings that may have occured are saved for each DBMS and per query.
The warning messages are generated if comparison of result sets detects any difference.
This is for inspection of problems.</p>
</div>
<div class="section" id="id23">
<h4>Initialization Scripts<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h4>
<p>If the result folder contains init scripts, they will be included in the latex report.</p>
</div>
<div class="section" id="bexhoma-workflow">
<h4>Bexhoma Workflow<a class="headerlink" href="#bexhoma-workflow" title="Permalink to this headline">¶</a></h4>
<p>If the result folder contains the configuration of a <a class="reference external" href="https://github.com/Beuth-Erdelt/Benchmark-Experiment-Host-Manager">bexhoma</a> workflow, it will be included in the latex report.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Patrick Erdelt.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>

  
  
    
   

</body>
</html>